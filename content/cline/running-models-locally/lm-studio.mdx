---
title: Lm Studio
description: Lm Studio 本地模型快速指南
---

# Lm Studio

Cline 本地 AI 模型运行环境 Lm Studio 快速上手指南。

## 🤖 Lm Studio 与 Cline 配置说明

使用 Lm Studio 可在本地运行 AI 模型并配合 Cline 使用。

## 📋 前置条件

- 支持 AVX2 指令集的 Windows、macOS 或 Linux 电脑
- 已在 VS Code 中安装 Cline

## 🚀 安装步骤

### 1. 安装 Lm Studio

- 访问 [lmstudio.ai](https://lmstudio.ai)
- 下载并安装适合你操作系统的版本

 
### 2. 启动 Lm Studio

- 打开已安装的 Lm Studio 应用
- 左侧有四个标签页：**Chat**、**Developer**（用于启动服务）、**My Models**（已下载模型）、**Discover**（添加新模型）

 
### 3. 下载模型

- 在"Discover"页面浏览模型
- 选择并下载你喜欢的模型
- 等待下载完成

 
### 4. 启动服务

- 切换到"Developer"标签页
- 打开服务开关至"Running"状态
- 注意：服务默认运行在 `http://localhost:1234`

 
### 5. 配置 Cline

1. 打开 VS Code
2. 点击 Cline 设置图标
3. 选择 "LM Studio" 作为 API 提供商
4. 从可用模型中选择你下载的模型

 
## ⚠️ 注意事项

- 使用 Cline 前请先启动 Lm Studio
- 保持 Lm Studio 在后台运行
- 首次下载模型可能需要几分钟，取决于模型大小
- 模型下载后会保存在本地

## 🔧 故障排查

1. 如果 Cline 无法连接 Lm Studio：
2. 检查 Lm Studio 服务是否已在"Developer"标签页运行
3. 确认已加载模型
4. 检查你的系统是否满足硬件要求

