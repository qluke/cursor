---
title: Ollama
description: Ollama 本地模型快速指南
---

# Ollama

Cline 本地 AI 模型运行环境 Ollama 快速上手指南。

## 📋 前置条件

- Windows、macOS 或 Linux 电脑
- 已在 VS Code 中安装 Cline

## 🚀 安装步骤

### 1. 安装 Ollama

- 访问 [ollama.com](https://ollama.com)
- 下载并安装适合你操作系统的版本

 
### 2. 选择并下载模型

- 在 [ollama.com/search](https://ollama.com/search) 浏览模型
- 选择模型并复制命令：

```shell
ollama run [模型名]
```

 
- 打开终端并运行命令，例如：

```shell
ollama run llama2
```

 
**✨ 现在你的模型已可在 Cline 中使用！**

### 3. 配置 Cline

1. 打开 VS Code
2. 点击 Cline 设置图标
3. 选择 "Ollama" 作为 API 提供商
4. 输入配置：
   - Base URL: `http://localhost:11434/`（默认值，可保持不变）
   - 从可用模型中选择你下载的模型

 
## ⚠️ 注意事项

- 使用 Cline 前请先启动 Ollama
- 保持 Ollama 在后台运行
- 首次下载模型可能需要几分钟

## 🔧 故障排查

如果 Cline 无法连接 Ollama：

1. 确认 Ollama 已启动
2. 检查 Base URL 是否正确
3. 确认模型已下载

需要更多信息？请阅读 [Ollama 官方文档](https://github.com/ollama/ollama/blob/main/docs/api.md)。

