---
title: 模型选择指南 
---

 

## 理解上下文窗口

---

可以把上下文窗口理解为 AI 助手的"工作内存"——类似于计算机的 RAM。它决定了模型在对话中能"记住"和处理的信息量，包括：

- 你的代码文件和对话内容
- 助手的回复
- 你提供的文档或其他上下文

上下文窗口以 token（英文大约 3/4 个单词）为单位，不同模型的窗口大小不同：

- Claude 3.5 Sonnet：200K tokens
- DeepSeek 系列：128K tokens
- Gemini Flash 2.0：1M tokens
- Gemini 1.5 Pro：2M tokens

当达到上下文窗口上限时，旧的信息会被移除，为新内容腾出空间——就像清理内存以运行新程序。这也是为什么 AI 助手有时会"忘记"早期对话内容。

Cline 通过上下文窗口进度条帮你管理这一限制，显示：

- 输入 token（你发送给模型的内容）
- 输出 token（模型生成的内容）
- 你已用上下文窗口的可视化进度
- 当前模型的总容量

 
这种可见性让你能更高效地与 Cline 协作，及时判断是否需要"清空上下文"或将任务拆分为更小的部分。

### 模型对比

## Cline 支持的主流大模型对比（2025年2月）

---

| 模型 | 输入费用* | 输出费用* | 上下文窗口 | 最佳用途 |
| ---- | --------- | --------- | ---------- | -------- |
| Claude 3.5 Sonnet | $3.00 | $15.00 | 200K | 代码实现&工具调用最佳 |
| DeepSeek R1 | $0.55 | $2.19 | 128K | 规划与推理冠军 |
| DeepSeek V3 | $0.14 | $0.28 | 128K | 性价比代码实现 |
| o3-mini | $1.10 | $4.40 | 200K | 灵活规划，强大推理 |
| Gemini Flash 2.0 | $0.00 | $0.00 | 1M | 全能型选手 |
| Gemini 1.5 Pro | $0.00 | $0.00 | 2M | 超大上下文处理 |

- *每百万 token 计费

### 2025 年推荐模型

1. **Claude 3.5 Sonnet**
    - 代码实现能力最强
    - 工具调用最可靠
    - 虽然价格高，但关键代码值得选择
2. **DeepSeek R1**
    - 规划与推理能力极佳
    - 价格实惠
3. **o3-mini**
    - 规划能力强，推理模式可调
    - 三种推理模式适配不同需求
    - 需 OpenAI Tier 3 API 权限
    - 200K 上下文窗口
4. **DeepSeek V3**
    - 代码实现稳定可靠
    - 日常编程首选
    - 实现性价比高
5. **Gemini Flash 2.0**
    - 超大 1M 上下文窗口
    - 速度与性能提升
    - 全能型表现

### 按模式推荐最佳模型（规划/执行）

  规划（Planning）

1. **DeepSeek R1**
    - 同类最佳推理能力
    - 擅长拆解复杂任务
    - 数学/算法规划强
    - MoE 架构提升推理
2. **o3-mini（高推理模式）**
    - 三档推理：
        - 高：复杂规划
        - 中：日常任务
        - 低：快速想法
    - 200K 上下文适合大型项目
3. **Gemini Flash 2.0**
    - 超大上下文适合复杂规划
    - 推理能力强
    - 多步骤任务表现好

  执行（Acting/编码）

1. **Claude 3.5 Sonnet**
    - 代码质量最佳
    - Cline 工具调用最可靠
    - 关键代码值得付费
2. **DeepSeek V3**
    - 代码质量接近 Sonnet
    - API 稳定性优于 R1
    - 日常编程首选
    - 工具调用能力强
3. **Gemini 1.5 Pro**
    - 2M 超大上下文
    - 复杂代码库表现好
    - API 稳定
    - 多文件理解能力强

### 关于本地模型的说明

虽然本地运行模型看似能节省成本，但目前我们不推荐在 Cline 中使用任何本地模型。[本地模型在 Cline 工具调用方面远不如云端模型](https://docs.cline.bot/running-models-locally/read-me-first)，通常只保留原模型 1-26% 的能力。例如 DeepSeek-R1 云端版参数量高达 671B，本地版大幅缩水，难以胜任复杂任务和工具调用。即使高配硬件（RTX 3070+，32GB+ 内存）也会遇到响应慢、工具执行不稳定、能力大幅下降等问题。最佳开发体验建议优先使用上述云端模型。

### 核心要点总结

1. **规划与执行模式很重要**：根据任务类型选模型
2. **实际表现优先于跑分**：关注 Cline 实际体验
3. **灵活切换**：规划与实现可用不同模型
4. **成本与质量权衡**：关键代码优先选高端模型
5. **备份方案**：API 异常时准备好备用模型
- *注：以上基于真实使用和社区反馈，非仅凭跑分。实际体验可能因人而异。Cline 支持的模型远不止于此。*

 